---
title: "DataFest 2018 Working with Large Data"
output: html_notebook
---

# Introduction

This presentation and all the supporting materials were originally developed by Colin Rundel. 

Datasets, presentation, source code: The slides and data can be downloaded in the following website
**www.stat.duke.edu/~oma9/datafest/df2018**

That data we will be using today is available via the ASA's 2009 Data Expo. The original data set was information on all domestic flights in the US between 1987 and 2008. We will be limiting ourselves to 2008 to keep the data sizes manageable.

packages you may need

- dplyr
- ggplot2
- data.table
- pryr
- microbenchmark
- rbenchmark
- profvis

-----

# Getting data into R
```{r}
library(tidyverse)
```

```{r}
setwd("/home/clint/Desktop")
dir()
```

use file.info to print out the information of files
```{r}
file.info(list.files("/home/clint/Desktop/data",full.names = TRUE))
```

here are examples that illustrate the time needed to read files
```{r}
system.time({flights = read.csv("2008.csv")})
```

```{r}
system.time({flights = read.csv("2008.csv.bz2")})
```


```{r}
system.time({flights = read.csv("2008.csv", comment.char = "", stringsAsFactors = FALSE)})
```

```{r}
system.time({flights = read_csv("data/2008.csv.bz2")})
```

you have to be careful about using fread. when you use it, the data is not loaded as dataframe. It is actually called data.table
```{r}
library(data.table)
system.time({flights = fread("data/2008.csv", showProgress = FALSE)})
```

if you want to convert it to data frame, just assign it to the class attributes. Note that the property will change when you do that
```{r}
class(flights) = "data.frame"
```

A lot of the difficulty and slowness of reading CSV files comes from trying to figure out what type each column should have. We can save ourselves this trouble later on by saving our data as a binary Rdata file.
```{r}
#there is a common mistake. For the first time, you can load it from csv file, 
#but then it is recommend to store and load it using RData file
save(flights, file="data/flights.Rdata")
system.time(load(file="data/flights.Rdata"))
```

-----

# A foray into algorithms

Lets imagine we have a 10 GB flat data file and that we want to select certain rows based on a given criteria. This requires a sequential read across the entire data set.

## Practical Advice

The development of databases (e.g. MySQL, Oracle, etc.) was to solve this problem: what do you do when data size >> available memory.
- Generally speaking they are a great tool but a bit of overkill for DataFest
- DataFest data is general on the scale of 1-2 GB
    - Small enough to fit in memory on most laptops
    - Large enough that you need to be careful (many operations will create hidden copys)

## Sampling FTW
    
**advice:**  
A simple but useful trick is to only work with the full data set when you absolutely have to. We are generally interested in the large scale patterns / features of the data and these are generally preserved by randomly sampling that data (this keeps statisticians employed).

```{r}
library(dplyr)
library(pryr)

set.seed(20180329)
flights_sub = flights %>% sample_frac(0.2) # 20% of the total rows

print(object.size(flights))
print(object.size(flights_sub))
```

## Aside - Deleting Objects
If you are not using the original object it is a good idea to delete it to free up memory (can always read it back in again later).

```{r}
rm(flights)
gc()

##            used  (Mb) gc trigger   (Mb)  max used   (Mb)
## Ncells  1062473  56.8    1770749   94.6   1770749   94.6
## Vcells 14146350 108.0  258829281 1974.8 577295819 4404.5
```

-----

# Other Useful Tools

## Progress bars (via dplyr)

Generally we want to avoid for loops, but that isn't always possible. For slow loops it is a good idea to track our progress (and know much time is left)

```{r}
library(dplyr)

p = progress_estimated(50, min_time = 0)
for(i in 1:50)
{
    # Calculate something compliated
    # ---------
    # your code
    # ---------
    
    # pause a little and print out the progress bar
    # allow you to visualize how fast is your code
    Sys.sleep(0.1)
    p$tick()$print() 
}
```


## Simple benchmarking

The simplest tool is base R's system.time which can be used to wrap any other call or calls.
```{r}
system.time(rnorm(1e6))
##    user  system elapsed 
##   0.077   0.004   0.085
```

```{r}
system.time(rnorm(1e4) %*% t(rnorm(1e4)))
##    user  system elapsed 
##   0.470   0.422   1.099
```

however, it is not efficient to put system.time in front of every code

## Better benchmarking - microbenchmark

We can do better (better precision) using the microbenchmark package
```{r}
library(microbenchmark)

d = abs(rnorm(1000))
r = microbenchmark(
      exp(log(d)/2),
      d^0.5,
      sqrt(d),
      times = 1000
    )
print(r)
## Unit: microseconds
##           expr    min      lq      mean  median      uq     max neval
##  exp(log(d)/2) 15.548 16.6960 20.498188 18.5840 20.9750  78.468  1000
##          d^0.5 26.327 26.8690 31.230703 28.8880 31.4465  79.104  1000
##        sqrt(d)  2.620  2.9245  5.312692  3.3375  7.0825 374.942  1000
```


```{r}
boxplot(r)
```

## Better benchmarking - rbenchmark

We can also use the rbenchmark package

the diff of rbenchmark and microbenchmark is that the rbenchmark can output the relative speed of each line.
```{r}
library(rbenchmark)

d = abs(rnorm(1000))
benchmark(
  exp(log(d)/2),
  d^0.5,
  sqrt(d),
  replications = 1000,
  order = "relative"
)

##            test replications elapsed relative user.self sys.self user.child sys.child
## 3       sqrt(d)         1000   0.006    1.000     0.005    0.001          0         0
## 1 exp(log(d)/2)         1000   0.029    4.833     0.025    0.001          0         0
## 2         d^0.5         1000   0.031    5.167     0.031    0.001          0         0
```

## Profiling

what if you want to know which part of the code is slowing down the whole process?  
=> use profvis
```{r}
library(profvis)

set.seed(20180329)
flights_small = flights %>% sample_n(100000)
```


```{r}
profvis({
  m = lm(AirTime ~ Distance, data = flights_small)
  plot(AirTime ~ Distance, data = flights_small)
  abline(m, col = "red")
})
```

-----

# General Advice and Pearls of Wisdom

## General Advice
- Sample, sample, sample
    - and also maybe subset (rows and columns)
- Save your code and important data objects regularly - bigger data can and will kill your R session
- If something is taking a long time - kill it and figure out why (or at least how long it will take)
- Vectorized >> *apply / map* >> for / while

## What's happening?

Make use of Activity Monitor (OSX) / Task Manager (Windows) to monitor CPU / Memory usage.

## Plotting

With large data sets overplotting is often a serious issue,

```{r}
#par(mfrow = c(2, 2))
####################

plot(AirTime ~ Distance, data = flights_small)

# ----
plot(AirTime ~ Distance, data = flights_small, 
     pch=16, 
     col=adjustcolor("black",alpha.f=0.01),
     cex=0.5)

# ----
ggplot(flights_small, aes(y=AirTime,x=Distance)) +
  geom_point(alpha=0.01, size=0.5)

# ----
ggplot(flights_small, aes(y=AirTime,x=Distance)) +
  geom_point(data=sample_n(flights,100)) +
  geom_density_2d(alpha=0.5) 

####################
#par(mfrow = c(1, 1))
```

## PDF Graphics

PDF and other SVG type plots are useful because you can adjust the scale and zoom, but when there are many many plot objects they can be painfully slow. For these cases consider creating an image (fixed resolution) based plot (e.g. png) instead.

```{r}
png("time_vs_dist.png", width=1024, height=800)
ggplot(flights_small, aes(y=AirTime,x=Distance)) +
  geom_point(alpha=0.01, size=0.5)
dev.off()
```


## Merging data

The different parts of the raw data or any additional outside data you find may need to be merged together. This can be down with dplyr's join functions or base R's merge - but it important to think about the nature of the relationship between the two data sets.

- **One to one** - each entry in A corresponds to either 0 or 1 entries in B
- **One to many or many to one** - each entry in A corresponds to 0 or more entries in B, or vice versa.
- **Many to many** - each entry in A corresponds to 0 or more entries in B and each entry in B corresponds to 0 or more entries in A.
    - you data can become very messy


## Why many to many merges are problematic
 
here is the case where you want to join the email and phone number. The repeat in the two datasets for joining will generate all the possible combinations
```{r}
# notice that the first dataset contains two Alice
addr = data.frame(name = c("Alice","Alice", "Bob","Bob"),
                  email= c("alice@company.com","alice@gmail.com",
                           "bob@company.com","bob@hotmail.com"),
                  stringsAsFactors = FALSE)

# the second dataset also contains two Alice
phone = data.frame(name = c("Alice","Alice", "Bob","Bob"),
                   phone= c("919 555-1111", "310 555-2222", 
                            "919 555-3333", "310 555-3333"),
                   stringsAsFactors = FALSE)

library(dplyr)
full_join(addr, phone, by="name")
##    name             email        phone
## 1 Alice alice@company.com 919 555-1111
## 2 Alice alice@company.com 310 555-2222
## 3 Alice   alice@gmail.com 919 555-1111
## 4 Alice   alice@gmail.com 310 555-2222
## 5   Bob   bob@company.com 919 555-3333
## 6   Bob   bob@company.com 310 555-3333
## 7   Bob   bob@hotmail.com 919 555-3333
## 8   Bob   bob@hotmail.com 310 555-3333
```


## Really really problematic

```{r}
A = data.frame(common="C", A_values=1:1000)
B = data.frame(common="C", B_values=1:1000)

inner_join(A,B) %>% tbl_df()
## Joining, by = "common"
## # A tibble: 1,000,000 x 3
##    common A_values B_values
##    <fctr>    <int>    <int>
##  1      C        1        1
##  2      C        1        2
##  3      C        1        3
##  4      C        1        4
##  5      C        1        5
##  6      C        1        6
##  7      C        1        7
##  8      C        1        8
##  9      C        1        9
## 10      C        1       10
## # ... with 999,990 more rows
```

## Note Joining
- mutating join
    - inner, left, right, full
- filtering join
    - semi, anti

## A brief note on algorithmic complexity

Some advice given by Mark Suchard at UCLA (unsure of who said this originally)

- Linear complexity (O(n)) - **Great**
    - Examples: Vectorized sqrt, lm
- Quadratic complexity (O(n2))(O(n2)) - **Pray**
    - Examples: Bubble sort, dist
- Cubic complexity (O(n3))(O(n3)) - **Give up**
    - Examples: %*%, solve, chol






















